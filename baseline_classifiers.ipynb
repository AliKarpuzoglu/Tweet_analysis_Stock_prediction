{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import naive_bayes, svm, metrics \n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# reset colwitdth options when running all cells \n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset (all tweets and corresponding stock prices)\n",
    "... and group by days / timestemps even groups weren't used yet in the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('processed_data/data_final_merged.json')\n",
    "# remove columns that were unexpectedly generated during saving\n",
    "# data.drop(columns=['level_0', 'index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>username</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>PriceUp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>[Muskwatchpic]</td>\n",
       "      <td>From SpaceX to Tesla, here are our biggest que...</td>\n",
       "      <td>Nerdist</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>312.00</td>\n",
       "      <td>320.53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>[Tesla]</td>\n",
       "      <td>#Tesla just released record delivery numbers f...</td>\n",
       "      <td>InsideEVs Forum</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>321.00</td>\n",
       "      <td>317.25</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>[]</td>\n",
       "      <td>Tesla struggles with Model 3 production  pic.t...</td>\n",
       "      <td>Automotive News</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>312.87</td>\n",
       "      <td>314.62</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>[munilandhttps]</td>\n",
       "      <td>Head of Puerto Rico electric utility says they...</td>\n",
       "      <td>Cate Long</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>316.62</td>\n",
       "      <td>316.58</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>[]</td>\n",
       "      <td>“Bırakın doğruları gelecek söylesin ve herkesi...</td>\n",
       "      <td>[n]Beyin</td>\n",
       "      <td>324</td>\n",
       "      <td>2</td>\n",
       "      <td>86</td>\n",
       "      <td>316.00</td>\n",
       "      <td>336.41</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-29</th>\n",
       "      <td>[]</td>\n",
       "      <td>You know Erin let us forget for a short period...</td>\n",
       "      <td>Darji</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>278.51</td>\n",
       "      <td>283.76</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-30</th>\n",
       "      <td>[]</td>\n",
       "      <td>Tesla Autopilot blamed for crash with parked p...</td>\n",
       "      <td>BBC News Technology</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>283.29</td>\n",
       "      <td>291.72</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>[Tesla]</td>\n",
       "      <td>Weekly #Tesla short update. $TSLA short intere...</td>\n",
       "      <td>Ihor Dusaniwsky</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>287.21</td>\n",
       "      <td>284.73</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>[1u]</td>\n",
       "      <td>Tesla and Elon Musk face tough questions from ...</td>\n",
       "      <td>Minnesota AFL-CIO</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>285.86</td>\n",
       "      <td>291.82</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-04</th>\n",
       "      <td>[]</td>\n",
       "      <td>It's great that Tesla can push rapid software ...</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>294.34</td>\n",
       "      <td>296.74</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   hashtags  \\\n",
       "timestamp                     \n",
       "2018-01-02   [Muskwatchpic]   \n",
       "2018-01-03          [Tesla]   \n",
       "2018-01-04               []   \n",
       "2018-01-05  [munilandhttps]   \n",
       "2018-01-08               []   \n",
       "...                     ...   \n",
       "2018-05-29               []   \n",
       "2018-05-30               []   \n",
       "2018-05-31          [Tesla]   \n",
       "2018-06-01             [1u]   \n",
       "2018-06-04               []   \n",
       "\n",
       "                                                         text  \\\n",
       "timestamp                                                       \n",
       "2018-01-02  From SpaceX to Tesla, here are our biggest que...   \n",
       "2018-01-03  #Tesla just released record delivery numbers f...   \n",
       "2018-01-04  Tesla struggles with Model 3 production  pic.t...   \n",
       "2018-01-05  Head of Puerto Rico electric utility says they...   \n",
       "2018-01-08  “Bırakın doğruları gelecek söylesin ve herkesi...   \n",
       "...                                                       ...   \n",
       "2018-05-29  You know Erin let us forget for a short period...   \n",
       "2018-05-30  Tesla Autopilot blamed for crash with parked p...   \n",
       "2018-05-31  Weekly #Tesla short update. $TSLA short intere...   \n",
       "2018-06-01  Tesla and Elon Musk face tough questions from ...   \n",
       "2018-06-04  It's great that Tesla can push rapid software ...   \n",
       "\n",
       "                       username  likes  replies  retweets    Open   Close  \\\n",
       "timestamp                                                                   \n",
       "2018-01-02              Nerdist     37        5        10  312.00  320.53   \n",
       "2018-01-03      InsideEVs Forum     11        1         5  321.00  317.25   \n",
       "2018-01-04      Automotive News      5        0         5  312.87  314.62   \n",
       "2018-01-05            Cate Long     17        4         9  316.62  316.58   \n",
       "2018-01-08             [n]Beyin    324        2        86  316.00  336.41   \n",
       "...                         ...    ...      ...       ...     ...     ...   \n",
       "2018-05-29                Darji     23        1         4  278.51  283.76   \n",
       "2018-05-30  BBC News Technology     11        3        11  283.29  291.72   \n",
       "2018-05-31      Ihor Dusaniwsky     12        4         8  287.21  284.73   \n",
       "2018-06-01    Minnesota AFL-CIO     19        0        10  285.86  291.82   \n",
       "2018-06-04               Forbes     52        5        22  294.34  296.74   \n",
       "\n",
       "            PriceUp  \n",
       "timestamp            \n",
       "2018-01-02     True  \n",
       "2018-01-03    False  \n",
       "2018-01-04     True  \n",
       "2018-01-05    False  \n",
       "2018-01-08     True  \n",
       "...             ...  \n",
       "2018-05-29     True  \n",
       "2018-05-30     True  \n",
       "2018-05-31    False  \n",
       "2018-06-01     True  \n",
       "2018-06-04     True  \n",
       "\n",
       "[103 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group data by day\n",
    "daily_data = data.groupby(data['timestamp'])\n",
    "daily_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-01-02 00:00:00')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get groups' names\n",
    "daily_data.groups.keys()\n",
    "groups = [name for name, _ in daily_data]\n",
    "groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>username</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>PriceUp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Muskwatchpic]</td>\n",
       "      <td>From SpaceX to Tesla, here are our biggest que...</td>\n",
       "      <td>Nerdist</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>312.0</td>\n",
       "      <td>320.53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Snapchat, Uber, Twitter, Facebook, Tesla, Goo...</td>\n",
       "      <td>Here's how old these companies will be turning...</td>\n",
       "      <td>Imran</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>312.0</td>\n",
       "      <td>320.53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Model3, Autopilot2, pasatealoelectrico, Tesla]</td>\n",
       "      <td>Primera prueba del @Tesla #Model3 en la nieve,...</td>\n",
       "      <td>PasatealoElectrico</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>312.0</td>\n",
       "      <td>320.53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>Know the whirr sound a Tesla makes?\\n\\nThat's ...</td>\n",
       "      <td>Elon Musk News</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>312.0</td>\n",
       "      <td>320.53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>In Norway, @Tesla finished Q4 with 3,753 Model...</td>\n",
       "      <td>Tesla Daily</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>312.0</td>\n",
       "      <td>320.53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            hashtags  \\\n",
       "0                                     [Muskwatchpic]   \n",
       "1  [Snapchat, Uber, Twitter, Facebook, Tesla, Goo...   \n",
       "2    [Model3, Autopilot2, pasatealoelectrico, Tesla]   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                                text            username  \\\n",
       "0  From SpaceX to Tesla, here are our biggest que...             Nerdist   \n",
       "1  Here's how old these companies will be turning...               Imran   \n",
       "2  Primera prueba del @Tesla #Model3 en la nieve,...  PasatealoElectrico   \n",
       "3  Know the whirr sound a Tesla makes?\\n\\nThat's ...      Elon Musk News   \n",
       "4  In Norway, @Tesla finished Q4 with 3,753 Model...         Tesla Daily   \n",
       "\n",
       "   likes  replies  retweets   Open   Close  PriceUp  \n",
       "0     37        5        10  312.0  320.53     True  \n",
       "1     53        7        41  312.0  320.53     True  \n",
       "2     23        0         6  312.0  320.53     True  \n",
       "3      8        0         5  312.0  320.53     True  \n",
       "4     28        0         6  312.0  320.53     True  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all tweets from the first day\n",
    "first_day_data = daily_data.get_group(groups[0])\n",
    "first_day_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first very simple classifier\n",
    "Use each tweet and predict whether it was written on a day where stock price has grown (PriceUp == True) or not\n",
    "\n",
    "As classificators use the algorithms learned in class: naive bayes and SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set size:\t 19991\n",
      "Test Set size:\t\t  3528\n"
     ]
    }
   ],
   "source": [
    "# generate the train and test sets\n",
    "tweets_train, tweets_test, labels_train, labels_test = train_test_split(data['text'], data['PriceUp'], \n",
    "                                                   test_size=0.15, random_state=333, shuffle=False)\n",
    "\n",
    "# now shuffle the data to decorrelate them for training and testing\n",
    "tweets_train, labels_train = shuffle(tweets_train, labels_train)\n",
    "tweets_test, labels_test = shuffle(tweets_test, labels_test)\n",
    "\n",
    "print('Training Set size:\\t', len(tweets_train))\n",
    "print('Test Set size:\\t\\t ', len(tweets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19991, 59290)\n",
      "Size of the vocabulary:  59290\n"
     ]
    }
   ],
   "source": [
    "# vectorize train and test data with TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_matrix = vectorizer.fit_transform(tweets_train)\n",
    "test_matrix = vectorizer.transform(tweets_test)\n",
    "print(train_matrix.shape)\n",
    "type(train_matrix)\n",
    "print(\"Size of the vocabulary: \", len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation:\n",
    "Our vector space is far higher than the number of training data. Therefore the classifier will for sure overfit and not generalize well to the test data at all. We can still use it as a simple baseline.\n",
    "\n",
    "BUT BEFORE...\n",
    "### Refit the vectorizer with most important tweets\n",
    "In order to decrease the vocabulary of the vectorizer, we first filter out the most important tweets (min. two retweets or five likes or five replies) and use them to determine the vocabulary of the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of considered tweets:  2684\n"
     ]
    }
   ],
   "source": [
    "vocab_data = data[(data['retweets']>25) &\n",
    "                  ((data['likes']>100) | (data['replies']>10))]\n",
    "\n",
    "# display whole tweet texts\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(\"Number of considered tweets: \", vocab_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary:  13399\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(dtype=np.float32)\n",
    "vectorizer.fit(vocab_data['text'])\n",
    "print(\"Size of the vocabulary: \", len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19991, 13399)\n"
     ]
    }
   ],
   "source": [
    "# now transform the train and test set with the limited vocabulary\n",
    "train_matrix = vectorizer.transform(tweets_train)\n",
    "test_matrix = vectorizer.transform(tweets_test)\n",
    "print(train_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have almost ten times more data than dimension in vector space. This is definitely better compared with having less data as dimensions, but is expected to still not be enough to train a good model. \n",
    "\n",
    "We still try it and use it as a simple baseline, that we'll try to overcome!\n",
    "Also an SVM is reported to be effective even when the number of dimensions is greater than the number of samples, so we definitely give it a try.\n",
    "\n",
    "Normalization of data may not be necessary, as a TfIdf vectorizer outputs vectors with values between 0 and 1. But we should better check the means. Having a unit variance will not work as we are dealing with a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and Max means:  0.0 0.05854584\n",
      "Min and Max after substracting the means:  -7.129554e-08 8.599791e-08\n"
     ]
    }
   ],
   "source": [
    "means = np.mean(train_matrix, axis=0)\n",
    "print('Min and Max means: ', np.matrix.min(means), np.matrix.max(means))\n",
    "train_matrix -= means\n",
    "means = np.mean(train_matrix, axis=0)\n",
    "print('Min and Max after substracting the means: ', np.matrix.min(means), np.matrix.max(means))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e-08 for the maximum mean are definitely better than 0.05. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use min-max normalization instead\n",
    "# doesn't work because of mins and maxs being sparse matrices\n",
    "mins = np.amin(train_matrix, axis=0)\n",
    "maxs = np.max(train_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_classifier = svm.LinearSVC(C=1.5, max_iter=int(1e8))\n",
    "svm_classifier.fit(train_matrix, labels_train)\n",
    "\n",
    "# Caution: you might not have enough memory to train the NB classifier\n",
    "nb_classifier = naive_bayes.GaussianNB()\n",
    "nb_classifier.fit(train_matrix, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if training was successful\n",
    "As we have not enough data, a working classifier should overfit to the training data and hence perfectly predict the labels of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \t\tSVM \t\tNaive Bayes\n",
      "Acc \t\t 0.825 \t\t 0.654\n",
      "Prec \t\t 0.826 \t\t 0.990\n",
      "Rec \t\t 0.832 \t\t 0.326\n",
      "FMeas \t\t 0.829 \t\t 0.490\n"
     ]
    }
   ],
   "source": [
    "# check if classifier has really overfitted to the data by testing it on the training data\n",
    "preds_svm = svm_classifier.predict(train_matrix)\n",
    "svm_acc = metrics.accuracy_score(labels_train, preds_svm)\n",
    "\n",
    "preds_nb = nb_classifier.predict(train_matrix)\n",
    "nb_acc = metrics.accuracy_score(labels_train, preds_nb)\n",
    "# A NB classifier could not be trained due to unfulfilled memory requirements\n",
    "# nb_acc = 0\n",
    "\n",
    "svm_prec, svm_rec, svm_fscore, svm_sup = \\\n",
    "metrics.precision_recall_fscore_support(labels_train, preds_svm, pos_label=True, average='binary')\n",
    "\n",
    "nb_prec, nb_rec, nb_fscore, nb_sup = \\\n",
    "metrics.precision_recall_fscore_support(labels_train, preds_nb, pos_label=True, average='binary')\n",
    "\n",
    "print('   \\t\\tSVM \\t\\tNaive Bayes')\n",
    "print('Acc \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_acc, nb_acc))\n",
    "print('Prec \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_prec, nb_prec))\n",
    "print('Rec \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_rec, nb_rec))\n",
    "print('FMeas \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_fscore, nb_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD: Check! Both classifiers reach an almost 100% accuracy on the training data. Therefore we can be sure, the classifier really learned a model based on the training data.\n",
    "\n",
    "UPDATED: Both classifiers reache a performance that is clearly over random guessing. Even not reaching 100% accuracy that would definitely show overfitting, we definitely see that our model has learned patterns in the training data. The usage of a simple linear SVM, no complex kernels, no hyperparameter tuning etc. does not allow to get better results.\n",
    "\n",
    "### Evaluate learned models on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \t\tSVM \t\tNaive Bayes\n",
      "Acc \t\t 0.496 \t\t 0.499\n",
      "Prec \t\t 0.504 \t\t 0.529\n",
      "Rec \t\t 0.477 \t\t 0.133\n",
      "FMeas \t\t 0.490 \t\t 0.212\n"
     ]
    }
   ],
   "source": [
    "# test the classifiers\n",
    "preds_svm = svm_classifier.predict(test_matrix)\n",
    "svm_acc = metrics.accuracy_score(labels_test, preds_svm)\n",
    "\n",
    "preds_nb = nb_classifier.predict(test_matrix.toarray())\n",
    "nb_acc = metrics.accuracy_score(labels_test, preds_nb)\n",
    "#nb_acc = 0\n",
    "\n",
    "svm_prec, svm_rec, svm_fscore, svm_sup = \\\n",
    "metrics.precision_recall_fscore_support(labels_test, preds_svm, pos_label=True, average='binary')\n",
    "\n",
    "nb_prec, nb_rec, nb_fscore, nb_sup = \\\n",
    "metrics.precision_recall_fscore_support(labels_test, preds_nb, pos_label=True, average='binary')\n",
    "\n",
    "print('   \\t\\tSVM \\t\\tNaive Bayes')\n",
    "print('Acc \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_acc, nb_acc))\n",
    "print('Prec \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_prec, nb_prec))\n",
    "print('Rec \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_rec, nb_rec))\n",
    "print('FMeas \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_fscore, nb_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "As expected, we get a classifier that is only as good as random guessing. Even not expecting better results, we tried out some different hyperparameters that have not improved the results. Trying out an SVM with polynomial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD (getting about 57% accuracy when shuffling the data during train-test-split): To make a statement about the results, we first have to look at the distribution of labels in the test dataset.\n",
    "An even simpler baseline we can use to compare our results with is a classifier that constantly predicts the class that is most common in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classifier that always predicts 'True' would get an accuracy of: 0.508\n"
     ]
    }
   ],
   "source": [
    "num_trues, num_falses = labels_test.value_counts()\n",
    "print(\"A classifier that always predicts 'True' would get an accuracy of: %.3f\" % (num_trues/labels_test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD (getting about 57% accuracy when shuffling the data during train-test-split): On the first sight, our classifier seems to have learned a very little bit, having an accuracy of 55.4 and 54.9 percent while the constant prediction would lead to 53.7 percent. In our case of predicting whether the stock price will close higher that it has opened based on a tweet, recall is much more important to us.\n",
    "\n",
    "Altogether the difference is too insignificant and is expected to be not reproducible when using different hyper parameters like the size of the training set, another random seed etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update**: After playing a bit with hyperparameters, we can say that the accuracy of our classifiers is always slightly above the constant value predictor. The SVM classifier always reaches a higher accuracy than the NB classifier as well as a higher Recall, which is especially important for our goal as we want to avoid false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** It looks like this very simple classifier already has learned some patterns in the data, which is unexpected, but can be explained as follows: \n",
    "\n",
    "- test and training data are expected to be correlated, as the test data contains tweets from days which we've already trained on. A better evaluation: use data of new days\n",
    "- Think about it again: As we're predicting only if a tweet was written on a good day for TSLA and do not consider the time a tweet was written at, it is likely to happen, that people write about the positive development of the stock price...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Update:\n",
    "After no longer shuffling the data on the train-test-split, the evaluation of our model results in the expected accuracy of below 50%. With shuffling the data before, the test set contained tweets from the same days that were already used during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
