{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import naive_bayes, svm, metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# reset colwitdth options when running all cells \n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset (all tweets and corresponding stock prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('processed_data/data_merged.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>text</th>\n",
       "      <th>username</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>PriceUp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>[Tesla, ModelS]</td>\n",
       "      <td>In the past 2 years, I've driven 18,823 miles ...</td>\n",
       "      <td>Ben Sullins ðŸ’ª</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>312.0</td>\n",
       "      <td>320.53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>[Tesla, P90D, Blog, Youtube]</td>\n",
       "      <td>Ya estamos en @louesfera probando un #Tesla #P...</td>\n",
       "      <td>Fco Javier</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>312.0</td>\n",
       "      <td>320.53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>[Snapchat, Uber, Twitter, Facebook, Tesla, Goo...</td>\n",
       "      <td>Here's how old these companies will be turning...</td>\n",
       "      <td>Imran</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>312.0</td>\n",
       "      <td>320.53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>[Muskwatchpic]</td>\n",
       "      <td>From SpaceX to Tesla, here are our biggest que...</td>\n",
       "      <td>Nerdist</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>312.0</td>\n",
       "      <td>320.53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>[Braunschweig, VW, Tesla]</td>\n",
       "      <td>In #Braunschweig produziert #VW seine Batterie...</td>\n",
       "      <td>HAZ</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>312.0</td>\n",
       "      <td>320.53</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp                                           hashtags  \\\n",
       "0 2018-01-02                                    [Tesla, ModelS]   \n",
       "1 2018-01-02                       [Tesla, P90D, Blog, Youtube]   \n",
       "2 2018-01-02  [Snapchat, Uber, Twitter, Facebook, Tesla, Goo...   \n",
       "3 2018-01-02                                     [Muskwatchpic]   \n",
       "4 2018-01-02                          [Braunschweig, VW, Tesla]   \n",
       "\n",
       "                                                text       username  likes  \\\n",
       "0  In the past 2 years, I've driven 18,823 miles ...  Ben Sullins ðŸ’ª    110   \n",
       "1  Ya estamos en @louesfera probando un #Tesla #P...     Fco Javier      2   \n",
       "2  Here's how old these companies will be turning...          Imran     53   \n",
       "3  From SpaceX to Tesla, here are our biggest que...        Nerdist     37   \n",
       "4  In #Braunschweig produziert #VW seine Batterie...            HAZ      5   \n",
       "\n",
       "   replies  retweets   Open   Close  PriceUp  \n",
       "0        6        10  312.0  320.53     True  \n",
       "1        1         2  312.0  320.53     True  \n",
       "2        7        41  312.0  320.53     True  \n",
       "3        5        10  312.0  320.53     True  \n",
       "4        3         2  312.0  320.53     True  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfIdf-Vectorize the data and apply PCA to reduce vector dimensionality \n",
    "Use each tweet and predict whether it was written on a day where stock price has grown (PriceUp == True) or not\n",
    "\n",
    "As classificators use the algorithms learned in class: naive bayes and SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4206"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the train and test sets\n",
    "tweets_train, tweets_test, labels_train, labels_test = train_test_split(data['text'], data['PriceUp'], \n",
    "                                                   test_size=0.15, random_state=333, shuffle=True)\n",
    "len(tweets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of a tweet vector is  22108\n",
      "The type of the train_matrix is  <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# vectorize train and test data with TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_matrix = vectorizer.fit_transform(tweets_train)\n",
    "test_matrix = vectorizer.transform(tweets_test)\n",
    "\n",
    "print(\"The dimension of a tweet vector is \", train_matrix.shape[1])\n",
    "print(\"The type of the train_matrix is \", type(train_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4206, 22108)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix into a numpy matrix\n",
    "train_matrix = train_matrix.todense()\n",
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Our vector space is far higher than the number of training data. Therefore the classifier will for sure overfit and not generalize well to the test data at all. To overcome this, we apply PCA and drastically reduce the dimensionality of the tweet vectors remaining only 90% of explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min/Max values before normalization: -1.4834715569020316, 64.84597092407806\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-8974c3d18f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# normalizer has to be fitted only on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Min/Max values before normalization: {np.min(train_matrix)}, {np.max(train_matrix)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 800\u001b[0;31m                     \u001b[0;34m\"Cannot center sparse matrices: pass `with_mean=False` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m                     \"instead. See docstring for motivation and alternatives.\")\n\u001b[1;32m    802\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives."
     ]
    }
   ],
   "source": [
    "# PCA requires a data normalization (zero mean and unit variance)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "normalizer = StandardScaler()\n",
    "print(f\"Min/Max values before normalization: {np.min(train_matrix)}, {np.max(train_matrix)}\")\n",
    "\n",
    "# Fit normalizer and transform train and test data\n",
    "# normalizer has to be fitted only on training data\n",
    "train_matrix = normalizer.fit_transform(train_matrix)\n",
    "test_matrix = normalizer.transform(test_matrix)\n",
    "print(f\"Min/Max values before normalization: {np.min(train_matrix)}, {np.max(train_matrix)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Unfortunately, sklearn cannot normalize a big sparse matrix to have zero mean (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler), which is mandatory to apply PCA. A simple alternative seems to be not around. Therefore we first stop further investigating this direction.\n",
    "\n",
    "#### Please ignore the following code\n",
    "Only remained in case we find an alternative way to reduce the data dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 22108)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = np.mean(train_matrix, axis=0)\n",
    "stds = np.std(train_matrix, axis=0)\n",
    "stds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min/Max values before normalization: 0.0, 1.0\n",
      "Min/Max values before normalization: -1.4834715569020316, 64.84597092407806\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-10\n",
    "print(f\"Min/Max values before normalization: {np.min(train_matrix)}, {np.max(train_matrix)}\")\n",
    "train_matrix = (train_matrix - means)/(stds+epsilon)\n",
    "print(f\"Min/Max values before normalization: {np.min(train_matrix)}, {np.max(train_matrix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_classifier = svm.LinearSVC(max_iter=int(1e6))\n",
    "svm_classifier.fit(train_matrix, labels_train)\n",
    "\n",
    "nb_classifier = naive_bayes.GaussianNB()\n",
    "nb_classifier.fit(train_matrix.toarray(), labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if training was successful\n",
    "As we have not enough data, a working classifier should overfit to the training data and hence perfectly predict the labels of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \t\tSVM \t\tNaive Bayes\n",
      "Acc \t\t 0.996 \t\t 0.977\n",
      "Prec \t\t 0.995 \t\t 1.000\n",
      "Rec \t\t 0.997 \t\t 0.957\n",
      "FMeas \t\t 0.996 \t\t 0.978\n"
     ]
    }
   ],
   "source": [
    "# check if classifier has really overfitted to the data by testing it on the training data\n",
    "preds_svm = svm_classifier.predict(train_matrix)\n",
    "svm_acc = metrics.accuracy_score(labels_train, preds_svm)\n",
    "\n",
    "preds_nb = nb_classifier.predict(train_matrix.toarray())\n",
    "nb_acc = metrics.accuracy_score(labels_train, preds_nb)\n",
    "\n",
    "\n",
    "svm_prec, svm_rec, svm_fscore, svm_sup = \\\n",
    "metrics.precision_recall_fscore_support(labels_train, preds_svm, pos_label=True, average='binary')\n",
    "\n",
    "nb_prec, nb_rec, nb_fscore, nb_sup = \\\n",
    "metrics.precision_recall_fscore_support(labels_train, preds_nb, pos_label=True, average='binary')\n",
    "\n",
    "print('   \\t\\tSVM \\t\\tNaive Bayes')\n",
    "print('Acc \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_acc, nb_acc))\n",
    "print('Prec \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_prec, nb_prec))\n",
    "print('Rec \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_rec, nb_rec))\n",
    "print('FMeas \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_fscore, nb_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check! Both classifiers reach an almost 100% accuracy on the training data. Therefore we can be sure, the classifier really learned a model based on the training data.\n",
    "\n",
    "### Test learned models on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \t\tSVM \t\tNaive Bayes\n",
      "Acc \t\t 0.569 \t\t 0.556\n",
      "Prec \t\t 0.584 \t\t 0.592\n",
      "Rec \t\t 0.599 \t\t 0.473\n",
      "FMeas \t\t 0.592 \t\t 0.526\n"
     ]
    }
   ],
   "source": [
    "# test the classifiers\n",
    "preds_svm = svm_classifier.predict(test_matrix)\n",
    "svm_acc = metrics.accuracy_score(labels_test, preds_svm)\n",
    "\n",
    "preds_nb = nb_classifier.predict(test_matrix.toarray())\n",
    "nb_acc = metrics.accuracy_score(labels_test, preds_nb)\n",
    "\n",
    "\n",
    "svm_prec, svm_rec, svm_fscore, svm_sup = \\\n",
    "metrics.precision_recall_fscore_support(labels_test, preds_svm, pos_label=True, average='binary')\n",
    "\n",
    "nb_prec, nb_rec, nb_fscore, nb_sup = \\\n",
    "metrics.precision_recall_fscore_support(labels_test, preds_nb, pos_label=True, average='binary')\n",
    "\n",
    "print('   \\t\\tSVM \\t\\tNaive Bayes')\n",
    "print('Acc \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_acc, nb_acc))\n",
    "print('Prec \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_prec, nb_prec))\n",
    "print('Rec \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_rec, nb_rec))\n",
    "print('FMeas \\t\\t {0:.3f} \\t\\t {1:.3f}'.format(svm_fscore, nb_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a statement about the results, we first have to look at the distribution of labels in the test dataset.\n",
    "An even simpler baseline we can use to compare our results with is a classifier that constantly predicts the class that is most common in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A classifier that always predicts 'True' would get an accuracy of: 0.521\n"
     ]
    }
   ],
   "source": [
    "num_trues, num_falses = labels_test.value_counts()\n",
    "print(\"A classifier that always predicts 'True' would get an accuracy of: %.3f\" % (num_trues/labels_test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the first sight, our classifier seems to have learned a very little bit, having an accuracy of 55.4 and 54.9 percent while the constant prediction would lead to 53.7 percent. In our case of predicting whether the stock price will close higher that it has opened based on a tweet, recall is much more important to us.\n",
    "\n",
    "Altogether the difference is too insignificant and is expected to be not reproducible when using different hyper parameters like the size of the training set, another random seed etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update**: After playing a bit with hyperparameters, we can say that the accuracy of our classifiers is always slightly above the constant value predictor. The SVM classifier always reaches a higher accuracy than the NB classifier as well as a higher Recall, which is especially important for our goal as we want to avoid false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** It looks like this very simple classifier already has learned some patterns in the data, which is unexpected, but can be explained as follows: \n",
    "\n",
    "- test and training data are expected to be correlated, as the test data contains tweets from days which we've already trained on. A better evaluation: use data of new days\n",
    "- Think about it again: As we're predicting only if a tweet was written on a good day for TSLA and do not consider the time a tweet was written at, it is likely to happen, that people write about the positive development of the stock price...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
